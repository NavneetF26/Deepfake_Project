import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.xception import Xception
from keras.models import Model
from keras.layers import Dense, Dropout, GlobalAveragePooling2D
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Check GPU
print("GPU available:", tf.config.list_physical_devices('GPU'))

# Paths
train_path = "/kaggle/input/deepfake-image-dataset/DeepFake/train"
test_path = "/kaggle/input/deepfake-image-dataset/DeepFake/test"
valid_path = "/kaggle/input/deepfake-image-dataset/DeepFake/validation"
# Parameters
img_size = 299
batch_size = 16
epochs = 10


# Data Generators
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    shear_range=0.1,
    brightness_range=[0.8, 1.2],
    horizontal_flip=True,
    fill_mode='nearest'
)

test_gen = ImageDataGenerator(rescale=1./255)
val_gen = ImageDataGenerator(rescale=1./255)

val_data = val_gen.flow_from_directory(
    valid_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=False
)

train_data = train_gen.flow_from_directory(
    train_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='binary'
)

test_data = test_gen.flow_from_directory(
    test_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=False
)

from keras.applications.xception import Xception
from keras.layers import Activation, Dense,GlobalAveragePooling2D, Dropout
from keras.models import Model
from keras.optimizers import SGD
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
base_model = Xception(weights='imagenet', include_top=False )
# add a global spatial average pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)


from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.optimizers import Adam

# Define the checkpoint callback to save the model at the end of every epoch
checkpoint = ModelCheckpoint(
    '/kaggle/working/model_epoch_{epoch:02d}.h5',  # Path where the model will be saved
    save_best_only=False,  # Save every epoch, not just the best
    save_weights_only=False,  # Save the full model (not just weights)
    verbose=1  # Display progress of saving the model
)

# Early stopping and reduce learning rate callbacks
earlystop = EarlyStopping(monitor='val_loss',
                          min_delta=0,
                          patience=epochs,
                          verbose=1,
                          restore_best_weights=True)

reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              factor=0.2,
                              patience=5,
                              verbose=1,
                              min_delta=0.0001)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), 
              loss='binary_crossentropy', 
              metrics=['accuracy'])

# Combine all callbacks into a single list
callbacks = [earlystop, reduce_lr, checkpoint]

# Train the model with the checkpoint callback and automatic progress bar
History = model.fit(
    train_data,
    epochs=epochs,
    validation_data=val_data,
    callbacks=callbacks,
    verbose=1,
    workers=4,
    use_multiprocessing=True
)
